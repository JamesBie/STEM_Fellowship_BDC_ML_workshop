{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning describes a collection of techniques that attempt to find structure in data.\n",
    "\n",
    "Examples include:\n",
    "\n",
    "* __Clustering__: attempts to divide data into groups based off a definition of similarity, such as distance in univariate or multidimensional space.\n",
    "* __Dimensionality Reduction__: Attempts to encode high dimensional data (e.g. tens of thousands of variables) into a smaller set of variables\n",
    "\n",
    "This is in contrast to supervised learning, where, given an input $x$ and output $y$ we attempt to learn a function $f(x)$ = $y$, where $x$ and $y$ can be scalars (numbers) , vectors (lines of numbers), or matrices (2D array of numbers). In unsupervised learning, we are only given $x$ and no output, and the goal is to find structure in a collection of data points.\n",
    "\n",
    "Here, we'll go over 2 Techniques: dimensionality reduction and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Often we can deal with datasets that exist in high dimensional spaces (e.g $d$ = 10,000, where $d$ is the number of variables in the data). The breast cancer dataset we shall explore has 30 dimensions for 'input data' features  plus 1 for 'target' label (i.e. whether or not the example is benign or malignant). Not only is it impossible for people to visualize data beyond 3 dimensions, but when we use supervised learning techniques such as K-nearest neighbor classification or an unsupervised method such as K-means clustering, the assumption that similar datapoints are closer to one another than dissimilar points in terms of Euclidean distance is violated<sup>1</sup>. The violation of this assumption is called the _curse of dimensionality_. A further discussion of this topic can be found [here](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html).\n",
    "\n",
    "What dimensionality reduction can accomplish is to pick up neat properties of your data in high dimensional space and express them in a much lower dimensional space. This allows an automated fashion of finding those neat properties while also expressing data in a way better suited for downstream applications such as clustering or classification. Examples include Princpal Components Analysis (PCA), Factor Analysis, Locally Linear Embedding, and more. Today we'll go over PCA.\n",
    "\n",
    "1. For 2D and 3D data, Euclidean distance is just the length of a line drawn between two points $p$ and $q$ if you plotted them in 2 or 3D, and its formula is $\\sqrt{\\sum_{d=1}^D (p_d - q_d)^2} $, where $d$ denotes an individual variable and $D$ is the total number of variables. Here's an example in 2D from Wikipedia:\n",
    "\n",
    "![Euclidean Distance in 2D (Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/5/55/Euclidean_distance_2d.svg)\n",
    "\n",
    "### Overview of PCA:\n",
    "\n",
    "Without diving into linear algebra, what PCA tries to do at the end of the day is express the data as weighted sums of the data's variables.\n",
    "\n",
    "The procedure is as follows:\n",
    "\n",
    "1. Standard Normalize Data. Part of the reason we do this is so the procedure doesn't give too much weight to certain variables based on certain variables having a wider range of values.\n",
    "\n",
    "    Formula is as follows: $b_{id} = (x_{id} - \\bar{x_d})/\\sigma_d$, where $i$ denotes sample, $d$ denotes variable,  $\\bar{x_d}$ denotes mean for the variable among all $x_{id}$, and $\\sigma_d$ denotes the standard deviation for variable d\n",
    "\n",
    "2. calculate a variable $v_1$ = $\\sum_{d=1}^{D} a_d b_d$ such that $Var[v_1]$ (variance of the new variable) is maximized, with the constraint that $\\sum_{d=1}^{D} a_d^2$ = 1\n",
    "\n",
    "3. repeat step 2, but subject to the constraint that the new variable, $v_2$ is uncorrelated with $v_1$, i.e correlation between the two variables = 0.\n",
    "\n",
    "4. Repeat step 3, again and again, subject to the constraint that any new variable $v_k$ computed for iteration $l$ is not correlated with any variables $v_{k'}$ where $k' < k$ (i.e not correlated with any previously computed variables).\n",
    "\n",
    "There's an upper limit on the amount of PCs that can be computed, but we're not discussing that today as it requires knowledge of linear algebra.\n",
    "\n",
    "For a more in depth discussion of PCA, here's a reference you can look into:\n",
    "\n",
    "* [Stanford Engineering Everywhere, CS229 PCA Notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes10.pdf)\n",
    "\n",
    "Let's dive into the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Breast Cancer Dataset (TODO: Implement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering (Note, skip hierarchical for time; TODO: Implement)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Suppose you want to discover if there are groups of samples that are similar to one another, but don't know where to start? Manually examining numbers and grouping things together is not something people are great at, unless it's points in 2D and 3D space. Even then, you want to have a definition of how you got those groups. Clustering refers to automated procedures by which we can find groups given a bunch of samples defined by numbers.\n",
    "\n",
    "### Kmeans\n",
    "\n",
    "One example of clustering is kmeans, which attempts to find groups of samples where samples in the same group are closer to one another than they are to other samples.\n",
    "\n",
    "The function it tries to minimize is as follows:\n",
    "\n",
    "$\\sum_{i=1}^{N}\\sum_{k=1}^K r_{ik} \\cdot dist(x_i, \\mu_k)^2$\n",
    "\n",
    "where $x_i$ is a datapoint, $r_{ik}$ = 1 if $x_i$ is in cluster $k$ and = 0 otherwise, and $dist(x_i, \\mu_k)$ is the Euclidean distance between the datapoint and the mean of all datapoints in cluster k.\n",
    "\n",
    "More details on kmeans algorithm [here](https://see.stanford.edu/materials/aimlcs229/cs229-notes7a.pdf).\n",
    "\n",
    "We can try clustering with a varying number of clusters. For this exercise, we're going to use the first 2 PCs we found in the breast cancer dataset, as it will be easier to visualize what kmeans is doing.\n",
    "\n",
    "Let's try k = 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the data's been partitioned into two clusters of samples that are close to one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try k = 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 3 groups of samples that looks close to one another.\n",
    "\n",
    "### Side Notes on Kmeans\n",
    "\n",
    "Kmeans starts with randomly assigned clusters, and tries to improve cluster assignment in an iterative process. We set a seed for a random number generator which ensures that, given a particular random seed, we get the same clustering result every time. Different start points can result in different solutions. If you really want to be rigorous about checking if the solutions that come out don't vary too much given different start points, you can run kmeans 100-1000 times with different seeds and calculate the similarity of solutions with the [adjusted rand index](https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index).\n",
    "\n",
    "Kmeans works best if data is distributed roughly spherically, i.e. the density of points emanating from the center of a given group doesn't change that much depending on the direction outward from the center of the cluster. Kmeans also doesn't work well when groups of samples have a curved distribution. Examples of how violations of this assumption can lead to bad clustering are given [here](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the number of clusters\n",
    "\n",
    "The question then arises: How many clusters is the best number? If we choose too few, we might not be capturing all of the cool structure in the data. \n",
    "\n",
    "We can use the silhouette score, which compares the the distance between samples in the same cluster to samples in the nearest cluster (more on the silhouette score [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).  Ideally, we want to maximize this score across all clusters, so we'll take the average silhouette score across all samples and make this the function we want to maximize.\n",
    "\n",
    "Let's see both the clusters that come out and the resulting scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Properties of Clusters\n",
    "\n",
    "We can do a quick visualization of how certain variables distribute in our clusters. (do visualization of 2-3 variables)\n",
    "\n",
    "If we want to be confident that these differences are 'real', we do statistical tests. Here, we'll compare (variable) between cluster 1 and all cluster 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "We did an initial data exploration of a breast cancer dataset with PCA, and learned how clustering can automatically group similar samples together. Now it's your turn to apply what you've learned to your own data and see if you can find any cool insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
